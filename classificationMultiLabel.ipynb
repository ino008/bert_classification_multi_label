{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir mult_classification_sample\n",
    "%cd ./mult_classification_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers==4.47.1 fugashi==1.4.0 ipadic==1.0.0 pytorch-lightning==1.8.6 unidic-lite==1.0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# 日本語の事前学習モデル\n",
    "MODEL_NAME = 'tohoku-nlp/bert-base-japanese-whole-word-masking'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassificationMultiLabel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        # model_name: Transformaers のモデル名\n",
    "        # num_labels: ラベルの数\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # BERT のロード\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "\n",
    "        # 線形変換を初期化\n",
    "        self.liner = torch.nn.Linear(\n",
    "            self.bert.config.hidden_size, num_labels\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        labels=None\n",
    "    ):\n",
    "        # データを入力し、BERT の最終層の出力を得る\n",
    "        bert_output = self.bert(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids = token_type_ids\n",
    "        )\n",
    "        last_hidden_state = bert_output.last_hidden_state\n",
    "\n",
    "        # [PAD] 以外のトークンで隠れ状態の平均をとる\n",
    "        average_hidden_state = \\\n",
    "            (last_hidden_state * attention_mask.unsqueeze(-1)).sum(1) \\\n",
    "            / attention_mask.sum(1, keepdim=True)\n",
    "\n",
    "        # 線形変換\n",
    "        scores = self.liner(average_hidden_state)\n",
    "\n",
    "        # 出力の形式を整える\n",
    "        output = { 'logits': scores }\n",
    "\n",
    "        # labels が入力に含まれていたら、損失を計算し、出力する\n",
    "        if labels is not None:\n",
    "            loss = torch.nn.BCEWithLogitsLoss()(scores, labels.float())\n",
    "            output['loss'] = loss\n",
    "        \n",
    "        # 属性でアクセスできるようにする\n",
    "        output = type ('bert_output', (object,), output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トークナイザのロード\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "# ネガ/ポジ の2クラスの複数クラス分類を想定\n",
    "bert_scml = BertForSequenceClassificationMultiLabel(MODEL_NAME, num_labels=2)\n",
    "bert_scml = bert_scml.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chABSA-dataset のダウンロード\n",
    "!wget https://s3-ap-northeast-1.amazonaws.com/dev.tech-sketch.jp/chakki/public/chABSA-dataset.zip\n",
    "# zipを展開\n",
    "!unzip chABSA-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_id = { 'negative': 0, 'neutral': 1, 'positive': 2  }\n",
    "\n",
    "dataset = []\n",
    "for file in glob.glob('chABSA-dataset/*.json'):\n",
    "    data = json.load(open(file))\n",
    "    # 各データから文章(text)を抜き出し、ラベル('labels')を作成\n",
    "    for sentence in data['sentences']:\n",
    "        text = sentence['sentence']\n",
    "        labels = [0, 0, 0]\n",
    "        for opinion in sentence['opinions']:\n",
    "            labels[category_id[opinion['polarity']]] = 1\n",
    "        sample = { 'text': text, 'labels': labels }\n",
    "        dataset.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各データの形式を整える\n",
    "max_length = 128\n",
    "dataset_for_loader = []\n",
    "for sample in dataset:\n",
    "    text = sample['text']\n",
    "    labels = sample['labels']\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True\n",
    "    )\n",
    "    encoding['labels'] = labels\n",
    "    encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n",
    "    dataset_for_loader.append(encoding)\n",
    "\n",
    "# データセットの分割\n",
    "random.shuffle(dataset_for_loader)\n",
    "n = len(dataset_for_loader)\n",
    "n_train = int(0.6*n)\n",
    "n_val = int(0.2*n)\n",
    "dataset_train = dataset_for_loader[:n_train]        # 学習用データ\n",
    "dataset_val = dataset_for_loader[n_train:n_val]     # 検証用データ\n",
    "dataset_test = dataset_for_loader[n_train+n_val:]   # テストデータ\n",
    "\n",
    "# データセットからデータローダーを作成\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train, batch_size=32, shuffle=True\n",
    ")\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=256)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassificationMultiLabel_pl(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model_name, num_labels, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.bert_scml = BertForSequenceClassificationMultiLabel(\n",
    "            model_name, num_labels=num_labels\n",
    "        )\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.bert_scml(**batch)\n",
    "        loss = output.loss\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.bert_scml(**batch)\n",
    "        loss = output.loss\n",
    "        self.log('val_loss', loss)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        labels = batch.pop('labels')\n",
    "        output = self.bert_scml(**batch)\n",
    "        scores = output.logits\n",
    "        labels_predicted = ( scores > 0 ).int()\n",
    "        num_correct = ( labels_predicted == labels ).all(-1).sum().item()\n",
    "        accuracy = num_correct / scores.size(0)\n",
    "        self.log('accuracy', accuracy)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_top_k=1,\n",
    "    save_weights_only=True,\n",
    "    dirpath='model/',\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    max_epochs=5,\n",
    "    callbacks = [checkpoint]\n",
    ")\n",
    "\n",
    "model = BertForSequenceClassificationMultiLabel_pl(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3,\n",
    "    lr=1e-5\n",
    ")\n",
    "trainer.fit(model, dataloader_train, dataloader_val)\n",
    "test = trainer.test(dataloaders=dataloader_test)\n",
    "print(f'Accuracy: {test[0][\"accuracy\"]:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
